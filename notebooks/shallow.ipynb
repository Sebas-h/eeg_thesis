{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.abspath(\"\"), '..', 'braindecode')))\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torch as th\n",
    "import numpy as np\n",
    "\n",
    "from braindecode.models.deep4 import Deep4Net\n",
    "from braindecode.models.util import to_dense_prediction_model\n",
    "from braindecode.experiments.experiment import Experiment\n",
    "from braindecode.experiments.monitors import LossMonitor, MisclassMonitor, \\\n",
    "    RuntimeMonitor, CroppedTrialMisclassMonitor\n",
    "from braindecode.experiments.stopcriteria import MaxEpochs, NoDecrease, Or\n",
    "from braindecode.datautil.iterators import CropsFromTrialsIterator\n",
    "from braindecode.models.shallow_fbcsp import ShallowFBCSPNet\n",
    "from braindecode.torch_ext.constraints import MaxNormDefaultConstraint\n",
    "from braindecode.torch_ext.util import set_random_seeds, np_to_var\n",
    "\n",
    "from braindecode.torch_ext.optimizers import AdamW\n",
    "from braindecode.torch_ext.schedulers import ScheduledOptimizer, CosineAnnealing\n",
    "from braindecode.datautil.iterators import get_balanced_batches\n",
    "from numpy.random import RandomState\n",
    "\n",
    "# log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcic_pickle_folder = os.path.abspath(os.path.join(os.path.abspath(\"\"), '..', 'data/pickled_bcic_iv_2a'))\n",
    "bcic_pickle_folder += \"/\"\n",
    "subject_id = 1  # 1-9\n",
    "\n",
    "# Load data:\n",
    "with open(f'{bcic_pickle_folder}{subject_id}_train_set.pickle', 'rb') as f:\n",
    "    train_set = pickle.load(f)\n",
    "with open(f'{bcic_pickle_folder}{subject_id}_valid_set.pickle', 'rb') as f:\n",
    "    valid_set = pickle.load(f)\n",
    "with open(f'{bcic_pickle_folder}{subject_id}_test_set.pickle', 'rb') as f:\n",
    "    test_set = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = False  # or do: torch.cuda.is_available()\n",
    "set_random_seeds(seed=20190706, cuda=cuda)  # Set seeds for python random module numpy.random and torch.\n",
    "\n",
    "# This will determine how many crops are processed in parallel:\n",
    "input_time_length = 1000\n",
    "n_classes = 4\n",
    "n_chans = int(train_set.X.shape[1])  # number of channels\n",
    "\n",
    "# final_conv_length determines the size of the receptive field of the ConvNet\n",
    "model = ShallowFBCSPNet(n_chans, n_classes, input_time_length=input_time_length,\n",
    "                            final_conv_length=30).create_network()\n",
    "\n",
    "to_dense_prediction_model(model)\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create cropped iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "592 predictions per input/trial\n"
     ]
    }
   ],
   "source": [
    "# Because cropped, number of predictions per input/trial has to be determined\n",
    "dummy_input = np_to_var(train_set.X[:1, :, :, None])  # a single trial, all channels, all measurements\n",
    "if cuda:\n",
    "    dummy_input = dummy_input.cuda()\n",
    "out = model(dummy_input)\n",
    "n_preds_per_input = out.cpu().data.numpy().shape[2]\n",
    "\n",
    "print(\"{:d} predictions per input/trial\".format(n_preds_per_input))\n",
    "\n",
    "# Set size of one training iteration\n",
    "batch_size = 60\n",
    "iterator = CropsFromTrialsIterator(batch_size=batch_size, input_time_length=input_time_length,\n",
    "                                   n_preds_per_input=n_preds_per_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "rng = RandomState((2018,8,7))\n",
    "#optimizer = AdamW(model.parameters(), lr=1*0.01, weight_decay=0.5*0.001) # these are good values for the deep model\n",
    "optimizer = AdamW(model.parameters(), lr=0.0625 * 0.01, weight_decay=0)\n",
    "\n",
    "# Need to determine number of batch passes per epoch for cosine annealing\n",
    "n_epochs = 5  # 30\n",
    "n_updates_per_epoch = len([None for b in iterator.get_batches(train_set, True)])\n",
    "scheduler = CosineAnnealing(n_epochs * n_updates_per_epoch)\n",
    "\n",
    "# schedule_weight_decay must be True for AdamW\n",
    "optimizer = ScheduledOptimizer(scheduler, optimizer, schedule_weight_decay=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Train  Loss: 6.56048\n",
      "Train  Accuracy: 28.3%\n",
      "Valid  Loss: 7.23501\n",
      "Valid  Accuracy: 22.4%\n",
      "Epoch 1\n",
      "Train  Loss: 6.02545\n",
      "Train  Accuracy: 38.3%\n",
      "Valid  Loss: 6.73328\n",
      "Valid  Accuracy: 25.9%\n",
      "Epoch 2\n",
      "Train  Loss: 4.02626\n",
      "Train  Accuracy: 39.1%\n",
      "Valid  Loss: 4.62523\n",
      "Valid  Accuracy: 29.3%\n",
      "Epoch 3\n",
      "Train  Loss: 2.43668\n",
      "Train  Accuracy: 43.0%\n",
      "Valid  Loss: 2.95319\n",
      "Valid  Accuracy: 34.5%\n",
      "Epoch 4\n",
      "Train  Loss: 1.44937\n",
      "Train  Accuracy: 47.8%\n",
      "Valid  Loss: 1.83830\n",
      "Valid  Accuracy: 34.5%\n"
     ]
    }
   ],
   "source": [
    "from braindecode.torch_ext.util import np_to_var, var_to_np\n",
    "import torch.nn.functional as F\n",
    "from numpy.random import RandomState\n",
    "import torch as th\n",
    "from braindecode.experiments.monitors import compute_preds_per_trial_from_crops\n",
    "\n",
    "rng = RandomState((2017,6,30))\n",
    "\n",
    "# for i_epoch in range(20):\n",
    "for i_epoch in range(n_epochs):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    for batch_X, batch_y in iterator.get_batches(train_set, shuffle=True):\n",
    "        net_in = np_to_var(batch_X)\n",
    "        if cuda:\n",
    "            net_in = net_in.cuda()\n",
    "        net_target = np_to_var(batch_y)\n",
    "        if cuda:\n",
    "            net_target = net_target.cuda()\n",
    "        # Remove gradients of last backward pass from all parameters\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(net_in)\n",
    "        # Mean predictions across trial\n",
    "        # Note that this will give identical gradients to computing\n",
    "        # a per-prediction loss (at least for the combination of log softmax activation\n",
    "        # and negative log likelihood loss which we are using here)\n",
    "        outputs = th.mean(outputs, dim=2, keepdim=False)\n",
    "        loss = F.nll_loss(outputs, net_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print some statistics each epoch\n",
    "    model.eval()\n",
    "    print(\"Epoch {:d}\".format(i_epoch))\n",
    "    for setname, dataset in (('Train', train_set),('Valid', valid_set)):\n",
    "        # Collect all predictions and losses\n",
    "        all_preds = []\n",
    "        all_losses = []\n",
    "        batch_sizes = []\n",
    "        for batch_X, batch_y in iterator.get_batches(dataset, shuffle=False):\n",
    "            net_in = np_to_var(batch_X)\n",
    "            if cuda:\n",
    "                net_in = net_in.cuda()\n",
    "            net_target = np_to_var(batch_y)\n",
    "            if cuda:\n",
    "                net_target = net_target.cuda()\n",
    "            outputs = model(net_in)\n",
    "            all_preds.append(var_to_np(outputs))\n",
    "            outputs = th.mean(outputs, dim=2, keepdim=False)\n",
    "            loss = F.nll_loss(outputs, net_target)\n",
    "            loss = float(var_to_np(loss))\n",
    "            all_losses.append(loss)\n",
    "            batch_sizes.append(len(batch_X))\n",
    "            \n",
    "        # Compute mean per-input loss\n",
    "        loss = np.mean(np.array(all_losses) * np.array(batch_sizes) /\n",
    "                       np.mean(batch_sizes))\n",
    "        print(\"{:6s} Loss: {:.5f}\".format(setname, loss))\n",
    "        \n",
    "        # Assign the predictions to the trials\n",
    "        preds_per_trial = compute_preds_per_trial_from_crops(all_preds,\n",
    "                                                          input_time_length,\n",
    "                                                          dataset.X)\n",
    "        # preds per trial are now trials x classes x timesteps/predictions\n",
    "        # Now mean across timesteps for each trial to get per-trial predictions\n",
    "        meaned_preds_per_trial = np.array([np.mean(p, axis=1) for p in preds_per_trial])\n",
    "        predicted_labels = np.argmax(meaned_preds_per_trial, axis=1)\n",
    "        accuracy = np.mean(predicted_labels == dataset.y)\n",
    "        print(\"{:6s} Accuracy: {:.1f}%\".format(setname, accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation - Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.53015\n",
      "Test Accuracy: 44.4%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "# Collect all predictions and losses\n",
    "all_preds = []\n",
    "all_losses = []\n",
    "batch_sizes = []\n",
    "\n",
    "for batch_X, batch_y in iterator.get_batches(test_set, shuffle=False):\n",
    "    net_in = np_to_var(batch_X)\n",
    "    if cuda:\n",
    "        net_in = net_in.cuda()\n",
    "    net_target = np_to_var(batch_y)\n",
    "    if cuda:\n",
    "        net_target = net_target.cuda()\n",
    "    outputs = model(net_in)\n",
    "    all_preds.append(var_to_np(outputs))\n",
    "    outputs = th.mean(outputs, dim=2, keepdim=False)\n",
    "    loss = F.nll_loss(outputs, net_target)\n",
    "    loss = float(var_to_np(loss))\n",
    "    all_losses.append(loss)\n",
    "    batch_sizes.append(len(batch_X))\n",
    "\n",
    "# Compute mean per-input loss\n",
    "loss = np.mean(np.array(all_losses) * np.array(batch_sizes) /\n",
    "               np.mean(batch_sizes))\n",
    "print(\"Test Loss: {:.5f}\".format(loss))\n",
    "\n",
    "# Assign the predictions to the trials\n",
    "preds_per_trial = compute_preds_per_trial_from_crops(all_preds,\n",
    "                                                  input_time_length,\n",
    "                                                  test_set.X)\n",
    "# preds per trial are now trials x classes x timesteps/predictions\n",
    "# Now mean across timesteps for each trial to get per-trial predictions\n",
    "meaned_preds_per_trial = np.array([np.mean(p, axis=1) for p in preds_per_trial])\n",
    "predicted_labels = np.argmax(meaned_preds_per_trial, axis=1)\n",
    "accuracy = np.mean(predicted_labels == test_set.y)\n",
    "print(\"Test Accuracy: {:.1f}%\".format(accuracy * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
